{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8749f70-f227-4017-96f5-6a871eb84e5b",
   "metadata": {},
   "source": [
    "# A Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05073cad-908b-400c-a00f-0bc599b5de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for multi-output regression\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import *\n",
    "from keras import Input\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468114f0-3a71-4fe0-a096-3d4d2fbb2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_type = 'PCAM'\n",
    "# sensor_type = 'Crumple'\n",
    "sensor_type = 'Planar'\n",
    "\n",
    "xls_train = pd.ExcelFile('../data/trajectory/turn/' + sensor_type + ' Sensor.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16e0d8-408f-40fb-9c77-6fd95e44bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_data(data):\n",
    "    mean = data.mean(axis=0)\n",
    "    std = data.std(axis=0)\n",
    "    data = (data - mean)/std\n",
    "    \n",
    "    return data, mean, std\n",
    "\n",
    "def preprocess_data(sheet):\n",
    "\n",
    "    # Normalize sensor readings\n",
    "    sheet[['1','2','3','4']] = sheet[['1','2','3','4']] \n",
    "    \n",
    "    # Start from (0,0) position\n",
    "    sheet['x'] = sheet['x'] - sheet['x'][0]\n",
    "    sheet['y'] = sheet['y'] - sheet['y'][0]\n",
    "    # Combine F and B, L and R\n",
    "    # sheet['F'] = sheet['F'] - sheet['B']\n",
    "    # sheet['L'] = sheet['L'] - sheet['R']\n",
    "    \n",
    "    # Down-sampling by moving average\n",
    "    new_data = sheet.values\n",
    "    new_data = signal.decimate(new_data, 10, axis=0)\n",
    "    new_data = signal.decimate(new_data, 10, axis=0)\n",
    "    # new_data = signal.decimate(new_data, 2, axis=0)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def augment_data(data):\n",
    "    start_idx = int(data.shape[0]/2)\n",
    "    data_repeat = data[start_idx:]\n",
    "    for i in range(10):\n",
    "        data = np.concatenate((data, data_repeat), axis=0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data(xls, sheets=None, features_all=None):\n",
    "    df = pd.read_excel(xls, sheet_name=sheets)\n",
    "    print(\"Read {} sheets from excel file\".format(len(df)))\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "    for sheet in df.values():\n",
    "        sheet.columns = features_all\n",
    "        new_data = preprocess_data(sheet)\n",
    "        # new_data = augment_data(new_data)\n",
    "        new_sheet = pd.DataFrame(new_data, columns = features_all)\n",
    "        new_df = pd.concat([new_df, new_sheet])\n",
    "        \n",
    "    # new_df.drop(['time','B','R'], axis=1, inplace=True)\n",
    "    print(\"DataFrame Shape: {} rows, {} columns\".format(*new_df.shape))\n",
    "    display(new_df.head())\n",
    "    \n",
    "    X = new_df[features_considered].values\n",
    "    y = new_df[outputs_considered].values\n",
    "    \n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    return X, y, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3a303-5abc-4e7d-ad44-6fb60fb2e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sheets = ['train 1', 'train 2', 'train 3', 'train 4']\n",
    "# test_sheets = ['test']\n",
    "train_sheets = ['train 2', 'train 3', 'train 4', 'test']\n",
    "test_sheets = ['train 1']\n",
    "\n",
    "features_all = ['t','1','2','3','4','x','y']\n",
    "# features_considered = ['t','1','2','3','4','F','L']\n",
    "# features_considered = ['t','1','2','3','4']\n",
    "features_considered = ['t','1','2']\n",
    "# features_considered = ['t','F','L']\n",
    "outputs_considered = features_all[-2:]\n",
    "\n",
    "# load train dataset\n",
    "X_train, y_train, mean_train, std_train = load_data(xls_train, train_sheets, features_all)\n",
    "print('Train data loaded')\n",
    "# load test dataset\n",
    "X_test, y_test, mean_test, std_test = load_data(xls_train, test_sheets, features_all)\n",
    "print('Test data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa57ccf-f392-47f9-a859-014a3ec572c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use \"lr_schedule\" to see which \"learning rate\" is optimum \n",
    "# Run the model with less epoch to visualize \"learning rate\" vs \"loss\"\n",
    "# lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "#                     lambda epoch: 1e-8 * 10**(epoch/20))\n",
    "# Optimizer and loos parameters\n",
    "# loss = tf.keras.losses.Huber()\n",
    "# optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "# optimizer = 'adam'\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_inputs,)))\n",
    "    model.add(BatchNormalization(name = 'batch_norm_0'))\n",
    "    model.add(Dense(16, name = 'dense_1', kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(BatchNormalization(name = 'batch_norm_1'))\n",
    "    # model.add(Dense(32, name = 'dense_2', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_2'))\n",
    "    # model.add(Dense(64, name = 'dense_3', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_3'))\n",
    "    # model.add(Dense(32, name = 'dense_4', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_4'))\n",
    "    model.add(Dense(16, name = 'dense_5', kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(BatchNormalization(name = 'batch_norm_5'))\n",
    "    model.add(Dense(8, name = 'dense_6', kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(BatchNormalization(name = 'batch_norm_6'))\n",
    "    model.add(Dense(n_outputs, name = 'dense_7'))\n",
    "    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mae', 'mse'])\n",
    "    \n",
    "    # model = Sequential()\n",
    "    # model.add(Input(shape=(n_inputs,)))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_0'))\n",
    "    # model.add(Dense(16, name = 'dense_1', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(0.30, name = 'dropout_1'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_1'))\n",
    "    # model.add(Dense(64, name = 'dense_2', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(0.20, name = 'dropout_2'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_2'))\n",
    "    # model.add(Dense(128, name = 'dense_3', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(0.20, name = 'dropout_3'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_3'))\n",
    "    # model.add(Dense(64, name = 'dense_4', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(0.20, name = 'dropout_4'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_4'))\n",
    "    # model.add(Dense(16, name = 'dense_5', kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(BatchNormalization(name = 'batch_norm_5'))\n",
    "    # model.add(Dense(n_outputs, name = 'dense_6'))\n",
    "    # model.compile(loss = \"mse\", optimizer=optimizer, metrics = ['mae', 'mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8227dbe-776a-40c7-8f9d-00e2ccbdeb4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# define model\n",
    "optimizer = 'adam'\n",
    "# optimizer = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "n_inputs, n_outputs = X_train.shape[1], y_train.shape[1]\n",
    "model = get_model(n_inputs, n_outputs, optimizer)\n",
    "print(model.summary())\n",
    "\n",
    "# Train\n",
    "# print(\"\\nTRAIN MODEL...\")\n",
    "modelstart = time.time()\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=EPOCHS, verbose=1)\n",
    "# history = model.fit(train_data, epochs=EPOCHS, validation_data=test_data, verbose=1)\n",
    "\n",
    "results = list()\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "for train_ix, test_ix in rkf.split(X_train):\n",
    "    ix_range = X_train.shape[0]\n",
    "    train_ix = train_ix[train_ix < ix_range]\n",
    "    test_ix = test_ix[test_ix < ix_range]\n",
    "    print(\"Filtered: TRAIN:\", train_ix.shape, \"TEST:\", test_ix.shape)\n",
    "    # prepare data\n",
    "    X_train_, X_test_ = X_train[train_ix], X_train[test_ix]\n",
    "    y_train_, y_test_ = y_train[train_ix], y_train[test_ix]\n",
    "    \n",
    "    # fit model\n",
    "    model.fit(X_train_, y_train_, verbose=1, epochs=EPOCHS)\n",
    "    # evaluate model on test set\n",
    "    mse = model.evaluate(X_test_, y_test_, verbose=1)\n",
    "    results.append(mse)\n",
    "\n",
    "print('Validation MSE: %.3f (%.3f)' % (mean(results), std(results))) \n",
    "\n",
    "print(\"\\nModel Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
    "model.save('mlp_full_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b16e47-0def-4bbc-9936-7f23f8e15a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_results(name, t, y_pred, y_test):\n",
    "    \n",
    "    FDE = np.linalg.norm(y_pred[-1,:] - y_test[-1,:])\n",
    "    print(\"Sheet No.\", name)\n",
    "    print(\"FDE =\", FDE)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    # plt.plot(np.array(y_pred[:,0]), np.array(y_pred[:,1]), 'r', label='predicted')\n",
    "    # plt.plot(np.array(y_test[:,0]), np.array(y_test[:,1]), 'g', label='real')\n",
    "    plt.plot(t, y_pred[:,0], 'r', label='predicted x')\n",
    "    plt.plot(t, y_pred[:,1], 'b', label='predicted y')\n",
    "    plt.plot(t, y_test[:,0], 'g', label='real x')\n",
    "    plt.plot(t, y_test[:,1], 'c', label='real y')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956145ec-70ce-41b9-84d5-af0edb9a449d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_excel(xls_train, sheet_name=test_sheets)\n",
    "\n",
    "print(\"Read {} sheets from excel file\".format(len(df_test)))\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../data/trajectory/' + sensor_type + '_result.xlsx', engine='openpyxl')\n",
    "\n",
    "results = list()\n",
    "for name, sheet in zip(df_test.keys(), df_test.values()):\n",
    "    print(\"DataFrame Shape: {} rows, {} columns\".format(*sheet.shape))\n",
    "    sheet.columns = features_all\n",
    "    new_data = preprocess_data(sheet)\n",
    "    new_sheet = pd.DataFrame(new_data, columns = features_all)\n",
    "    \n",
    "    t = new_sheet['t']\n",
    "    X_test = new_sheet[features_considered].values\n",
    "    y_test = new_sheet[outputs_considered].values\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "    plot_test_results(name, t, y_pred, y_test)\n",
    "\n",
    "    # evaluate model on test set\n",
    "    mse = model.evaluate(X_test, y_test, verbose=1)\n",
    "    results.append(mse)\n",
    "    \n",
    "    combined = np.concatenate([t[:, np.newaxis], y_test, y_pred], axis = 1)\n",
    "    df = pd.DataFrame(combined, columns = ['t', 'real_x', 'real_y', 'pred_x', 'pred_y'])\n",
    "    df.to_excel(writer, index=False, sheet_name=name)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "\n",
    "print('Test MSE: %.3f (%.3f)' % (mean(results), std(results))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b33ac3-fd69-4220-9fec-7351c6f1c80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edb6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
